{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnsiaNijas/ADS1/blob/main/sentiment_analysis_glove_bilstm_structured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b30a20e0",
      "metadata": {
        "id": "b30a20e0"
      },
      "source": [
        "# Sentiment Analysis with Pretrained GloVe Embeddings and LSTM on the IMDB Dataset\n",
        "This tutorial demonstrates how to perform sentiment analysis on the IMDB dataset using pretrained GloVe embeddings and a Bidirectional LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5458263",
      "metadata": {
        "id": "d5458263"
      },
      "source": [
        "## 1. Introduction\n",
        "Sentiment analysis is a critical natural language processing (NLP) task that determines whether a piece of text expresses a positive, negative, or neutral sentiment. This tutorial explores how to combine GloVe embeddings with LSTM to understand the sentiment in text effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb8c179",
      "metadata": {
        "id": "ecb8c179"
      },
      "source": [
        "## 2. Setup\n",
        "Import the required libraries and packages for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a79cc3",
      "metadata": {
        "id": "73a79cc3"
      },
      "source": [
        "## 3. Load Data\n",
        "Load the IMDB dataset using TensorFlow Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c639864",
      "metadata": {
        "id": "1c639864"
      },
      "source": [
        "## 4. Preprocess Data\n",
        "Prepare the data for training by tokenizing text, creating sequences, and padding them to uniform lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fddb1ee",
      "metadata": {
        "id": "1fddb1ee"
      },
      "source": [
        "## 5. Build Model\n",
        "Define the architecture of the sentiment analysis model using GloVe embeddings and Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a609a09",
      "metadata": {
        "id": "0a609a09"
      },
      "source": [
        "## 6. Train and Evaluate\n",
        "Train the model on the prepared data and evaluate its performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a4820e",
      "metadata": {
        "id": "41a4820e"
      },
      "source": [
        "## 7. Export Model\n",
        "Save the trained model for future inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a071dd07",
      "metadata": {
        "id": "a071dd07"
      },
      "source": [
        "## 8. Visualizations\n",
        "Generate plots to visualize the training and validation accuracy and loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cc09df",
      "metadata": {
        "id": "a0cc09df"
      },
      "source": [
        "## 9. Conclusion\n",
        "Summarize the results and discuss possible future improvements to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06db1609",
      "metadata": {
        "id": "06db1609"
      },
      "source": [
        "## **4. Code Walkthrough**\n",
        "### **Setup: Preparing the Environment**\n",
        "\n",
        "In this step, we prepare the environment by importing the necessary libraries and dependencies required for the tutorial. Each library serves a specific purpose in the workflow:\n",
        "\n",
        "1. **`numpy`**:\n",
        "   - Used for numerical computations, such as creating and manipulating arrays.\n",
        "   - Essential for working with the embedding matrix for GloVe.\n",
        "\n",
        "2. **`tensorflow`**:\n",
        "   - The primary framework used to build, train, and evaluate machine learning models.\n",
        "   - Provides utilities for neural network layers, optimization algorithms, and training workflows.\n",
        "\n",
        "3. **`tensorflow_datasets`**:\n",
        "   - A module that simplifies dataset loading and preprocessing.\n",
        "   - It will be used to import the IMDB dataset, which is pre-split into training and testing sets.\n",
        "\n",
        "4. **`tensorflow.keras.preprocessing.text`**:\n",
        "   - Provides tools to process textual data by tokenizing words into sequences of integers.\n",
        "\n",
        "5. **`tensorflow.keras.preprocessing.sequence`**:\n",
        "   - Used to pad or truncate sequences to ensure they have uniform length, making them suitable for input to the model.\n",
        "\n",
        "6. **`os`**:\n",
        "   - Enables interaction with the operating system.\n",
        "   - Will be used to load the GloVe embeddings from local storage.\n",
        "\n",
        "By importing these libraries, we set up the foundational tools needed to complete the rest of the tutorial, such as data preparation, model building, training, and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524f060a",
      "metadata": {
        "id": "524f060a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7e51f1",
      "metadata": {
        "id": "6c7e51f1"
      },
      "source": [
        "## Load the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc6080c",
      "metadata": {
        "id": "ecc6080c"
      },
      "outputs": [],
      "source": [
        "# Load IMDB dataset\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d31da45",
      "metadata": {
        "id": "5d31da45"
      },
      "source": [
        "## Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de702fae",
      "metadata": {
        "id": "de702fae"
      },
      "outputs": [],
      "source": [
        "# Extract sentences and labels\n",
        "train_sentences, train_labels = [], []\n",
        "test_sentences, test_labels = [], []\n",
        "\n",
        "for s, l in train_data:\n",
        "    train_sentences.append(s.numpy().decode('utf8'))\n",
        "    train_labels.append(l.numpy())\n",
        "\n",
        "for s, l in test_data:\n",
        "    test_sentences.append(s.numpy().decode('utf8'))\n",
        "    test_labels.append(l.numpy())\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ea8206",
      "metadata": {
        "id": "75ea8206"
      },
      "source": [
        "## Tokenize and Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40abdff",
      "metadata": {
        "id": "a40abdff"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 20000\n",
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Pad sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae48ca6",
      "metadata": {
        "id": "9ae48ca6"
      },
      "source": [
        "## Load Pretrained GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4558fb",
      "metadata": {
        "id": "dd4558fb"
      },
      "outputs": [],
      "source": [
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip -d ./glove.6B/\n",
        "\n",
        "# Load embeddings into a dictionary\n",
        "embeddings_index = {}\n",
        "with open('./glove.6B/glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c1f131",
      "metadata": {
        "id": "42c1f131"
      },
      "source": [
        "## Create Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62be26c5",
      "metadata": {
        "id": "62be26c5"
      },
      "outputs": [],
      "source": [
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cc0059",
      "metadata": {
        "id": "60cc0059"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce463a9f",
      "metadata": {
        "id": "ce463a9f"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397a963e",
      "metadata": {
        "id": "397a963e"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a6e138",
      "metadata": {
        "id": "28a6e138"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "history = model.fit(\n",
        "    train_padded,\n",
        "    train_labels,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=(test_padded, test_labels),\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbda6401",
      "metadata": {
        "id": "dbda6401"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc5d55d",
      "metadata": {
        "id": "9bc5d55d"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_padded, test_labels)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbdc0ce",
      "metadata": {
        "id": "6fbdc0ce"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dab7629",
      "metadata": {
        "id": "2dab7629"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('sentiment_analysis_glove_bilstm.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eebde13",
      "metadata": {
        "id": "5eebde13"
      },
      "source": [
        "## Load and Use the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bd0886",
      "metadata": {
        "id": "d9bd0886"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('sentiment_analysis_glove_bilstm.h5')\n",
        "\n",
        "# Predict on new data\n",
        "sample_text = [\"The movie was fantastic! I really enjoyed it.\"]\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "prediction = loaded_model.predict(sample_padded)\n",
        "print(f'Prediction: {prediction[0][0]:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}